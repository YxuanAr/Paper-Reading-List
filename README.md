# Paper-Reading-List
This is a paper reading list for myself. I am focusing on the multimodal alignment and also interest in other area.

## Vision Language Base Model 
- \[2024.6.11\] \[Auto-regressive\] \[<font color=red>arxiv</font>\] [Generative Multimodal Models are In-Context Learners (EMU2).](https://arxiv.org/pdf/2312.13286) (BAAI,THU,PKU)
- \[2024.6.11\] \[ICL\] \[<font color=red>arxiv</font>\] [What Makes Multimodal In-Context Learning Work?](https://arxiv.org/pdf/2404.15736) (Sorbonne Universite)
- \[2024.6.11\] \[ICL\] \[<font color=red>ICLR2024</font>\] [Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning.](https://arxiv.org/pdf/2310.00647) (Sorbonne Universite)
- \[2024.6.11\] \[Understanding\] \[<font color=red>arxiv</font>\] [On the Out-Of-Distribution Generalization of Multimodal Large Language Models.](https://arxiv.org/pdf/2402.06599) (THU)
- \[2024.6.11\] \[ICL\] \[<font color=red>arxiv</font>\] [Understanding and Improving In-Context Learning on Vision-language Models.](https://arxiv.org/pdf/2311.18021) (LMU,Oxford)

## Point Cloud Base Model
- \[Z-orderï¼ŒDecoder-only\] \[NIPS2023\] \[2024.6.14\] [PointGPT: Auto-regressively Generative Pre-training from Point Clouds.](https://arxiv.org/pdf/2305.11487) (BIT, PKU)
- \[Z-order\] \[CVPR2024*\] \[2024.6.14\] [Point Transformer V3: Simpler, Faster, Stronger.](https://arxiv.org/pdf/2312.10035) (HKU, SHAILAB)
- \[Oct-order\] \[SIGGRAPH2023\] \[2024.6.14\] [OctFormer: Octree-based Transformers for 3D Point Clouds.](https://arxiv.org/pdf/2305.03045) (PKU)

## Outdoor Scene and Autonomous Driving
- \[Completion\] \[CVPR2024*\] \[2024.6.13\] [PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness.](https://arxiv.org/pdf/2312.02158) (TUM)
- \[LLM Agent\] \[CVPR2024\] \[2024.6.13\] [Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents.](https://arxiv.org/pdf/2402.05746) (SHAILAB)
- \[World Model\] \[NIPS2024\] \[2024.6.13\] [Enhancing End-to-End Autonomous Driving with Latent World Model.](https://arxiv.org/pdf/2406.08481v1) (CAS)

